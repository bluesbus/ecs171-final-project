{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install libraries. SpaCy is for NLP pipeline and Sense2Vec is for token similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install sense2vec\n",
    "%pip install spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this if you don't have `ys-reviews-restaurants.json` already!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "reviews = pd.read_json('data/ys-reviews-with-categories.json')\n",
    "\n",
    "restaurants = reviews.loc[reviews.category == \"restaurant\"]\n",
    "restaurants.drop(columns=['category'], inplace=True)\n",
    "restaurants.to_json('data/ys-reviews-restaurants.json', orient='records')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this command to download the default spaCy pipeline. If this doesn't work, try running in the command line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python -m spacy download encore_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go to this link to download a zip archive of the Reddit Sense2Vec model: https://github.com/explosion/sense2vec/releases/download/v1.0.0/s2v_reddit_2015_md.tar.gz\n",
    "Make sure the folder `s2v_old` is in your directory!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start here if you've already done set up!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tok2vec', 'tagger', 'lower_case_lemmas', 'parser', 'attribute_ruler', 'lemmatizer']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "from spacy import Language\n",
    "from sense2vec import Sense2Vec\n",
    "# \n",
    "s2v = Sense2Vec().from_disk(\"s2v_old\")\n",
    "# Create a pipe that converts lemmas to lower case:\n",
    "@Language.component(\"lower_case_lemmas\")\n",
    "def lower_case_lemmas(doc) :\n",
    "    for token in doc :\n",
    "        token.lemma_ = token.lemma_.lower()\n",
    "    return doc\n",
    "# Initialize default spaCy pipeline\n",
    "nlp = spacy.load('en_core_web_sm', disable=['ner'])\n",
    "# lower_case_lemmas to pipeline\n",
    "nlp.add_pipe(factory_name=\"lower_case_lemmas\", after=\"tagger\")\n",
    "# Sanity check to make sure we have the right pipeline order\n",
    "print(nlp.pipe_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the reviews into a spaCy doc object. This takes ~45 minutes to run because it is tokenizing, parsing, lemmatizing, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Pipe restaurant review text into spaCy pipeline\n",
    "# Each review is a \"doc\" in \"docs\"\n",
    "reviews = pd.read_json('data/ys-reviews-restaurants.json', orient='records')\n",
    "docs = list(nlp.pipe(reviews['text'].to_list()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `topicDetection()` for food and service. `topic_list` contains similar words that describe the topic. Sense2Vec takes the average of the vector representations of each word. This defines a vector that is centered around the topic region of of vector space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect if a topic defined by topic_list is present in a sentence (span from spaCy doc)\n",
    "# If a doc has n sentences, return a list of n booleans, where each index represent a topic present or not\n",
    "# pos is a list of parts of speech to consider from doc\n",
    "# thresh is a threshold for cosine similarity. If similarity > threshold, topic is present\n",
    "def topicDetection(sentence, topic_list : list[str], pos : list[str], thresh) -> list[int]:\n",
    "    indices = []\n",
    "    for i, token in enumerate(sentence):\n",
    "      # Construct string to pass to Sense2Vec\n",
    "      s = token.lemma_ + \"|\" + token.pos_\n",
    "      # Only consider tokens that Sense2Vec model knows and are from specified part of speech\n",
    "      if (s in s2v and token.pos_ in pos) and (s2v.similarity(s, topic_list) > thresh):\n",
    "        indices.append(i)\n",
    "    # return a list of indices where topic was detected\n",
    "    return indices\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `separateTopicsDetection()` on location, clean, and price topics. These topics are described by disjoint concepts that don't create a meaningful average vector. For example, \"clean\" and \"dirty\" are opposite ideas. `separateTopicsDetection()` looks for tokens that are similar to at least word in `topics_list`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Operates like TopicDetection, except looks or matches to each string in topics_list seperately\n",
    "# Instead of averaging their vector representations\n",
    "def seperateTopicsDetection(sentence, topics_list : list[str], thresh, exclude_pos = []) -> list[int]:\n",
    "    indices = []\n",
    "    for i, token in enumerate(sentence):\n",
    "      # Skip token if explicitly told to ignore part of speech\n",
    "      if token.pos_ in exclude_pos:\n",
    "        continue\n",
    "      # Construct string to pass to Sense2Vec\n",
    "      s = token.lemma_ + \"|\" + token.pos_\n",
    "      # Only consider tokens that Sense2Vec model knows\n",
    "      if s in s2v:\n",
    "        # Add to indices list if token matches at least one topic from topic_list\n",
    "        for topic in topics_list:\n",
    "          if s2v.similarity(s, topics_list) > thresh:\n",
    "            indices.append(i)\n",
    "            break\n",
    "    return indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detect Food topic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform `topicDetection()` on each sentence of a doc in docs. `topicDetection()` returns a list of indices where topic was detected in the sentence. Record the token lemma, doc index, sentence index, and token index in `food_hits` list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sense2Vec will compute an average vector from the vector representation of these tokens\n",
    "food = [\"food|NOUN\", \"pizza|NOUN\", \"meal|NOUN\", \"taco|NOUN\", \"chinese|ADJ\", \"mexican|ADJ\", \"sushi|NOUN\", \"bone|NOUN\", \"drink|NOUN\", \"pho|NOUN\", \"curry|NOUN\", \"coffee|NOUN\", \"teriyaki|NOUN\"]\n",
    "food_hits = []\n",
    "for i, doc in enumerate(docs):\n",
    "  for j, sentence in enumerate(doc.sents):\n",
    "    # \n",
    "    for k in topicDetection(sentence, food, [\"NOUN\", \"ADJ\"], 0.6):\n",
    "      # for each token where the food topic is detected\n",
    "      # record lemma, doc index, sentence index, and token index\n",
    "      food_hits.append([sentence[k].lemma_ , i, j, k])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert `food_hits` to a dataframe and save to JSON file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "food_hits = pd.DataFrame(data=food_hits, columns=['lemma', 'doc_index', 'sentence_index', 'token_index'])\n",
    "food_hits.to_json('data/topics/food-hits-restaurant-reviews.json', orient='records')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detect Service Topic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same process as the food topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "service = [\"waiter|NOUN\", \"staff|NOUN\", \"service|NOUN\", \"employee|NOUN\"]\n",
    "service_hits = []\n",
    "for i, doc in enumerate(docs):\n",
    "  for j, sentence in enumerate(doc.sents):\n",
    "    for k in topicDetection(sentence, service, [\"NOUN\", \"ADJ\"], 0.7):\n",
    "      # for each token where the food topic is detected\n",
    "      # record lemma, doc index, sentence index, and token index\n",
    "      service_hits.append([sentence[k].lemma_ , i, j, k])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Issue: the word \"restaurant\" is very similar to the service topic, but shouldn't be included. Filter out instances of restaurant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove = [\"restaurant\", \"restraunt\", \"restaraunt\"]\n",
    "service_hits = pd.DataFrame(data=service_hits, columns=['lemma', 'doc_index', 'sentence_index', 'token_index'])\n",
    "# Remove \"restaurant\" or any typos from service hits\n",
    "service_hits = service_hits[~((service_hits['lemma'] == \"restaurant\") | (service_hits['lemma'] == \"restraunt\") | (service_hits['lemma'] == \"restaraunt\"))]\n",
    "service_hits.to_json('data/topics/service-hits-restaurant-reviews.json', orient='records')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detect Location Topic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `seperateTopicsDetection()` to detect if tokens match any of the tokens in `location` list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "location = [\"crowded|ADJ\", \"atmosphere|NOUN\", \"quiet|ADJ\", \"interior|NOUN\", \"music|NOUN\", \"environment|NOUN\", \"space|NOUN\", \"vibe|NOUN\", \"location|NOUN\"]\n",
    "location_hits = []\n",
    "for i, doc in enumerate(docs):\n",
    "  for j, sentence in enumerate(doc.sents):\n",
    "    for k in seperateTopicsDetection(sentence, location, 0.67):\n",
    "      # for each token where the food topic is detected\n",
    "      # record lemma, doc index, sentence index, and token index\n",
    "      location_hits.append([sentence[k].lemma_ , i, j, k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "location_hits = pd.DataFrame(data=location_hits, columns=['lemma', 'doc_index', 'sentence_index', 'token_index'])\n",
    "location_hits = location_hits[~((location_hits['lemma'] == \"especially\"))]\n",
    "location_hits.to_json('data/topics/location-hits-restaurant-reviews.json', orient='records')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detect Clean Topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean = [\"clean|ADJ\", \"dirty|ADJ\", \"fly|NOUN\", \"cockroach|NOUN\", \"filthy|ADJ\", \"spotless|ADJ\"]\n",
    "clean_hits = []\n",
    "for i, doc in enumerate(docs):\n",
    "  for j, sentence in enumerate(doc.sents):\n",
    "    for k in seperateTopicsDetection(sentence, clean, 0.7):\n",
    "      # for each token where the food topic is detected\n",
    "      # record lemma, doc index, sentence index, and token index\n",
    "      clean_hits.append([sentence[k].lemma_ , i, j, k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_hits = pd.DataFrame(data=clean_hits, columns=['lemma', 'doc_index', 'sentence_index', 'token_index'])\n",
    "clean_hits.to_json('data/topics/clean-hits-restaurant-reviews.json', orient='records')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detect Price Topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price = [\"cheap|ADJ\", \"expensive|ADJ\", \"price|NOUN\", \"worth|NOUN\", \"payment|NOUN\", \"tip|NOUN\"]\n",
    "price_hits = []\n",
    "for i, doc in enumerate(docs):\n",
    "  for j, sentence in enumerate(doc.sents):\n",
    "    # exclude verbs like \"pay\" or \"buy\"\n",
    "    for k in seperateTopicsDetection(sentence, price, 0.7, [\"VERB\"]):\n",
    "      # for each token where the food topic is detected\n",
    "      # record lemma, doc index, sentence index, and token index\n",
    "      price_hits.append([sentence[k].lemma_ , i, j, k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_hits = pd.DataFrame(data=price_hits, columns=['lemma', 'doc_index', 'sentence_index', 'token_index'])\n",
    "# clean_hits = clean_hits[~((location_hits['lemma'] == \"especially\"))]\n",
    "price_hits.to_json('data/topics/price-hits-restaurant-reviews.json', orient='records')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
